{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf4084b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4fce24",
   "metadata": {},
   "source": [
    "# Extracting Data From A Given URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60dcbb5",
   "metadata": {},
   "source": [
    "As most people will have experienced when developing a use case or even trying to practise coding, one of the most difficult stages of the task at hand is being able to get enough data that can be used to train your models or identify whether or not your potential solution is robust"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6442900c",
   "metadata": {},
   "source": [
    "### Webscraping\n",
    "\n",
    "Web scraping is the process of extracting data from websites. It has become an important tool for businesses and researchers to gather data on products, services, and trends, among other things. In recent years, there has been a growing interest in using web scraping to generate text embeddings, which are numerical representations of words or sentences that capture the meaning of the text. These embeddings can be used for a variety of natural language processing (NLP) tasks such as sentiment analysis, language translation, and text classification. Web scraping techniques can be used to collect large amounts of text data from websites, which can then be processed and used to generate high-quality text embeddings.\n",
    "\n",
    "\n",
    "For the purpose of this training, we will be scraping data from the Accenture careers website https://www.accenture.com/us-en/careers. This is publicly available information that we can send to OpenAI without being too concerned regrading the privacy concerns surrounding sending customer data to OpenAI's API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0bec7e",
   "metadata": {},
   "source": [
    "### Step 1: Loading the relevant libraries\n",
    "\n",
    "To get started running this code, we first need to load all of the relevant packages we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4a9025",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import re\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import deque\n",
    "from html.parser import HTMLParser\n",
    "from urllib.parse import urlparse\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "import traceback\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be7060f",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = 'INSERT_YOUR_YOUR_API_KEY_HERE'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6da0d5",
   "metadata": {},
   "source": [
    "### Step 2: Outline the relevant functions to be executed within the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dfcc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTTP_URL_PATTERN = r'^http[s]*://.+'\n",
    "\n",
    "class HyperlinkParser(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hyperlinks = []\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        attrs = dict(attrs)\n",
    "        if tag == \"a\" and \"href\" in attrs:\n",
    "            self.hyperlinks.append(attrs[\"href\"])\n",
    "\n",
    "def get_hyperlinks(url):\n",
    "    try:\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            if not response.info().get('Content-Type').startswith(\"text/html\"):\n",
    "                return []\n",
    "            html = response.read().decode('utf-8')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return []\n",
    "\n",
    "    parser = HyperlinkParser()\n",
    "    parser.feed(html)\n",
    "    return parser.hyperlinks\n",
    "\n",
    "def get_domain_hyperlinks(domain, url):\n",
    "    clean_links = []\n",
    "    for link in set(get_hyperlinks(url)):\n",
    "        clean_link = None\n",
    "\n",
    "        if re.search(HTTP_URL_PATTERN, link):\n",
    "            url_obj = urlparse(link)\n",
    "            if url_obj.netloc == domain:\n",
    "                clean_link = link\n",
    "        else:\n",
    "            if link.startswith(\"/\"):\n",
    "                link = link[1:]\n",
    "            elif link.startswith(\"#\") or link.startswith(\"mailto:\") or link.startswith(\"javascript:\") or 'eDelivery' in link:\n",
    "                continue\n",
    "            clean_link = \"https://\" + domain + \"/\" + link\n",
    "\n",
    "        if clean_link is not None:\n",
    "            if clean_link.endswith(\"/\"):\n",
    "                clean_link = clean_link[:-1]\n",
    "            clean_links.append(clean_link)\n",
    "\n",
    "    return list(set(clean_links))\n",
    "\n",
    "def crawl_with_root(root_url, limit_domain):\n",
    "    domain = urlparse(root_url).netloc\n",
    "    queue = deque([root_url])\n",
    "    seen = set([root_url])\n",
    "\n",
    "    if not os.path.exists(\"accenture_text\"):\n",
    "        os.mkdir(\"accenture_text\")\n",
    "\n",
    "    while queue:\n",
    "        url = queue.pop()\n",
    "        if not url.startswith(limit_domain):\n",
    "            continue\n",
    "        print(url)\n",
    "\n",
    "        if 'eDelivery' in url:\n",
    "            continue\n",
    "\n",
    "        path = urlparse(url).path\n",
    "        fname = 'accenture_text/' + path.replace(\"/\", \"_\") + \".txt\"\n",
    "\n",
    "        try:\n",
    "            if not os.path.exists(fname):\n",
    "                if url.endswith(\".pdf\"):\n",
    "                    with open(\"temp.pdf\", \"wb\") as f:\n",
    "                        f.write(requests.get(url).content)\n",
    "                    with pdfplumber.open(\"temp.pdf\") as pdf:\n",
    "                        text = \"\"\n",
    "                        for page in pdf.pages:\n",
    "                            text+= page.extract_text()\n",
    "                else:\n",
    "                    soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n",
    "                    text = soup.get_text()\n",
    "                with open(fname, \"w\") as f:\n",
    "                    f.write(text)\n",
    "\n",
    "            for link in get_domain_hyperlinks(domain, url):\n",
    "                if link not in seen:\n",
    "                    queue.append(link)\n",
    "                    seen.add(link)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            traceback.print_exc()\n",
    "\n",
    "def remove_newlines(serie):\n",
    "    serie = serie.str.replace('\\n', ' ')\n",
    "    serie = serie.str.replace('\\\\n', ' ')\n",
    "    serie = serie.str.replace('  ',' ')\n",
    "    serie = serie.str.replace('  ',' ')\n",
    "    return serie         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff47fe09",
   "metadata": {},
   "source": [
    "Now for a given URL we can extract all of the text from it and all of the assiciated hyperlinks "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaf0ce2",
   "metadata": {},
   "source": [
    "We then run the following code to scrape text data from a folder named \"accenture_text\", which contains multiple files that had been extracted when we passed the url through the 'crawl' function. The text data is then processed and stored in a pandas DataFrame named 'df'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379d5e93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "crawl_with_root(\"https://www.accenture.com/us-en/careers\", \"https://www.accenture.com/us-en/careers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14576dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=[]\n",
    "for file in os.listdir(\"accenture_text\"):\n",
    "    with open(\"accenture_text/\"+file, \"r\") as f:\n",
    "        text = f.read()\n",
    "        texts.append( (file[:-4].replace('-',' ').replace('_', ' ').replace('#update',''), text) )\n",
    "df = pd.DataFrame(texts, columns = ['fname', 'text'])\n",
    "\n",
    "df['text'] = df.fname + \". \" + remove_newlines(df.text)\n",
    "df.to_csv('accenture_careers_scraper.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea31cf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
